--- ./Project_FARSI_orig/design_utils/components/krnel.py
+++ ./Project_FARSI/design_utils/components/krnel.py
@@ -9,14 +9,16 @@
 from design_utils.components.workload import *
 from design_utils.components.mapping import *
 from design_utils.components.scheduling import *
+from typing import Tuple, List, Dict
 import operator
 from collections import OrderedDict
 from collections import defaultdict
+from settings import config
 from design_utils.common_design_utils import  *
 import warnings
 import queue
 from collections import deque
-
+import sys
 
 # This is a proprietary Queue class. Not used at the moment, but possible will be
 # used for Queue modeling
@@ -111,7 +113,9 @@
 
 # class for getting relevant metrics (latency, energy, ...) for the task
 class KernelStats:
-    def __init__(self):
+    def __init__(self, is_dummy, task_name):
+        self.task_name = task_name
+        self.is_dummy = is_dummy
         self.latency = 0
         self.area= 0
         self.power = 0
@@ -132,6 +136,7 @@
                                                                              # blocks for different durations become the kernel
                                                                              # bottleneck
         self.block_phase_energy_dict = {}
+        self.arrival_time = 0
         self.starting_time = 0  # when a kernel starts
         self.completion_time = 0  # when a kernel completes
         self.latency_till_earliest_child_start = 0
@@ -216,7 +221,10 @@
 
             sorted_block_duration_bottleneck = OrderedDict(
                 sorted(block_duration_bottleneck.items(), key=operator.itemgetter(1)))
-            sorted_normalized = [(key, 100*value / sum(sorted_block_duration_bottleneck.values())) for key, value in sorted_block_duration_bottleneck.items()]
+            if self.is_dummy:
+                sorted_normalized = [(list(block_duration_bottleneck.keys())[0], 100.)]
+            else:
+                sorted_normalized = [(key, 100*value / sum(sorted_block_duration_bottleneck.values())) for key, value in sorted_block_duration_bottleneck.items()]
 
             # for latency, we need to zero out the rest since there is only one bottleneck through out each phase
             non_bottleneck_blocks =[]
@@ -226,6 +234,7 @@
                 non_bottleneck_blocks.append(block)
             for block in non_bottleneck_blocks:
                 sorted_normalized.append((block, 0))
+            # print(f"?? Sorted normalized latencies: [{[(b.instance_name, perc) for b, perc in sorted_normalized]}]")
             return sorted_normalized
 
     # get the block bottlenecks (from power perspective) sorted
@@ -262,7 +271,7 @@
 
     # get the block bottlenecks (from the metric of interest perspective) sorted
     # phase: Simulation phases
-    def get_block_sorted(self, metric="latency", phase="all"):
+    def get_block_bottleneck_sorted(self, metric="latency", phase="all"):
         if metric == "latency":
             return self.get_block_latency_sorted(phase)
         elif metric == "power":
@@ -313,38 +322,67 @@
 
     # sets power for the entire kernel and also per blocks hosting the kernel
     def set_power(self):
+        # print("self.phase_energy_dict:")
+        # pprint(self.phase_energy_dict)
+        # print("self.phase_latency_dict:")
+        # pprint(self.phase_latency_dict)
+        
         # get energy first
         sorted_listified_phase_latency_dict = sorted(self.phase_latency_dict.items(), key=operator.itemgetter(0))
         sorted_durations = [duration for phase, duration in sorted_listified_phase_latency_dict]
         sorted_phase_latency_dict = collections.OrderedDict(sorted_listified_phase_latency_dict)
         sorted_listified_phase_energy_dict = sorted(self.phase_energy_dict.items(), key=operator.itemgetter(0))
         sorted_phase_energy_dict = collections.OrderedDict(sorted_listified_phase_energy_dict)
-        phase_bounds_lists = slice_phases_with_PWP(sorted_phase_latency_dict)
 
         # calculate power
         power_list = []  # list of power values collected based on the power collection freq
-        for lower_bnd, upper_bnd in phase_bounds_lists:
-            if sum(sorted_durations[lower_bnd:upper_bnd]) > 0:
-                power_list.append(
-                    sum(list(sorted_phase_energy_dict.values())[lower_bnd:upper_bnd]) / sum(sorted_durations[lower_bnd:upper_bnd]))
-            else:
-                power_list.append(0)
+        if config.PCP is not None: # TODO enable for all runs, this is just for safety around paper deadline
+            phase_bounds_lists = slice_phases_with_PWP(sorted_phase_latency_dict)
+            for lower_bnd, upper_bnd in phase_bounds_lists:
+                if sum(sorted_durations[lower_bnd:upper_bnd]) > 0:
+                    power_list.append(
+                        sum(list(sorted_phase_energy_dict.values())[lower_bnd:upper_bnd]) / sum(sorted_durations[lower_bnd:upper_bnd]))
+                else:
+                    power_list.append(0)
+        else:
+            # simpler way of calculating power
+            for phase in sorted_phase_latency_dict.keys():
+                latency_this_phase = sorted_phase_latency_dict[phase] 
+                if latency_this_phase == 0.0:
+                    power_this_phase = 0.0
+                else:
+                    power_this_phase = sorted_phase_energy_dict[phase] / latency_this_phase
+                power_list.append(power_this_phase)
+            
         self.power = max(power_list)
+        # print(f"set_power {self.task_name} power_list: {power_list}")
+        # print(f"set_power {self.task_name} power: {self.power}")
 
         # now calculate the above per block
         blck_pwr_list = defaultdict(list)
         for block, phase_energy_dict in self.block_phase_energy_dict.items():
             sorted_listified_phase_energy_dict = sorted(phase_energy_dict.items(), key=operator.itemgetter(0))
             sorted_phase_energy_dict = collections.OrderedDict(sorted_listified_phase_energy_dict)
-            for lower_bnd, upper_bnd in phase_bounds_lists:
-                if sum(sorted_durations[lower_bnd:upper_bnd]) > 0:
-                    blck_pwr_list[block].append(
-                        sum(list(sorted_phase_energy_dict.values())[lower_bnd:upper_bnd]) / sum(sorted_durations[lower_bnd:upper_bnd]))
-                else:
-                    blck_pwr_list[block].append(0)
+            if config.PCP is not None: # TODO enable for all runs, this is just for safety around paper deadline
+                for lower_bnd, upper_bnd in phase_bounds_lists:
+                    if sum(sorted_durations[lower_bnd:upper_bnd]) > 0:
+                        blck_pwr_list[block].append(
+                            sum(list(sorted_phase_energy_dict.values())[lower_bnd:upper_bnd]) / sum(sorted_durations[lower_bnd:upper_bnd]))
+                    else:
+                        blck_pwr_list[block].append(0)
+            else:
+                # simpler way of calculating power
+                for phase in sorted_phase_latency_dict.keys():
+                    latency_this_phase = sorted_phase_latency_dict[phase] 
+                    if latency_this_phase == 0.0:
+                        power_this_phase = 0.0
+                    else:
+                        power_this_phase = sorted_phase_energy_dict[phase] / latency_this_phase
+                    blck_pwr_list[block].append(power_this_phase)
 
         for blck in blck_pwr_list.keys() :
             self.blck_pwr[blck] = max(blck_pwr_list[blck])
+            # print(f"set_power {self.task_name} blck_pwr[{blck.instance_name}]: {self.blck_pwr[blck]}")
 
     def set_area(self, area):
         self.area = area
@@ -414,29 +452,45 @@
 # This class emulates the a task within the workload.
 # The difference between kernel and task is that kernel is a simulation construct containing timing/energy/power information.
 class Kernel:
-    def __init__(self, task_to_blocks_map: TaskToBlocksMap): #, task_to_pe_block_schedule: TaskToPEBlockSchedule):
+    def __init__(self, task_to_blocks_map: TaskToBlocksMap, blocks:List[Block], task_to_mappable_pe_map:Dict[str, List[str]], mem_subscribers:Dict[Block,Dict[Tuple[str,str],float]]):
+        self.mem_subscribers = mem_subscribers
+        # map of all the memories that this kernel is writing to, to a tuple of the write work, estimated write latency and the list of all the children tasks of this task
+        # this is updated by get_estimated_latency only once
+        self.mem_write_lat_children_names:Dict[Block,Tuple[float, float, List[str]]] = {}
+
+        self.dropped = False
+        self.blocks = blocks
+        self.task_to_mappable_pe_map = task_to_mappable_pe_map
         # constructor argument vars, any changes to these need to initiate a reset
         self.__task_to_blocks_map = task_to_blocks_map  # mapping of the task to blocks
+        # support for staggered DAG mode
+        self.dag_id = task_to_blocks_map.task.dag_id
+        self.parent_graph_arr_time = task_to_blocks_map.task.parent_graph_arr_time
+        if config.STAGGERED_GRAPH_MODE:
+            assert self.parent_graph_arr_time != None
+            assert self.dag_id != None
         self.kernel_total_work = {}
+        self.task = self.__task_to_blocks_map.task
         #self.kernel_total_work = self.__task_to_blocks_map.task.get_self_task_work()  # work (number of instructions for the task)
-        self.kernel_total_work["execute"] =  self.__task_to_blocks_map.task.get_self_total_work("execute")
-        self.kernel_total_work["read"] = self.__task_to_blocks_map.task.get_self_total_work("read")
-        self.kernel_total_work["write"] =self.__task_to_blocks_map.task.get_self_total_work("write")
+        self.kernel_total_work["execute"] =  self.task.get_self_total_work("execute")
+        self.kernel_total_work["read"] = self.task.get_self_total_work("read")
+        self.kernel_total_work["write"] =self.task.get_self_total_work("write")
         self.data_work_left = {}
-        self.max_iteration_ctr = self.iteration_ctr = self.__task_to_blocks_map.task.iteration_ctr
+        self.max_iteration_ctr = self.iteration_ctr = self.task.iteration_ctr
         # status vars
         self.cur_phase_bottleneck = ""  # which phase does the bottleneck occurs
         self.block_att_work_rate_dict = defaultdict(dict)  # for the kernel, block and their attainable work rate.
                                                            # attainable work rate is peak work rate (BW or IPC) of the block
                                                            # but attenuated as it is being shared among multiple kernels/fronts
 
-        self.type = self.__task_to_blocks_map.task.get_type()
-        self.throughput_info = self.__task_to_blocks_map.task.get_throughput_info()
+        self.type = self.task.get_type()
+        self.throughput_info = self.task.get_throughput_info()
         self.operating_state = "none"
         self.block_path_dir_phase_latency = {}
         self.path_dir_phase_latency = {}
         self.pathlet_phase_latency_dict = {}
-        self.stats = KernelStats()
+        self.task_name = self.task.name
+        self.stats = KernelStats(self.task.is_task_dummy(), self.task_name)
         self.workload_pe_total_work, self.workload_fraction, self.pe_s_work_left, self.progress, self.status = [None]*5
         self.block_dir_work_left = defaultdict(dict)  # block and the direction (write/read or loop) and how much work is left for the
                                                       # the kernel on this block
@@ -451,33 +505,55 @@
         self.SOC_type = ""
         self.SOC_id = ""
         self.set_SOC()
-        self.task_name = self.__task_to_blocks_map.task.name
+        self.is_dummy = self.task.is_task_dummy_but_not_souurce()
 
         # The variable shows what power_knob is being used!
         # 0 means baseline, and any other number is a DVFS/power_knob mode
         self.power_knob_id = 0
-        self.starting_time = 0
-        self.completion_time = 0
+        
+        self.arrived = False
+        self.arrival_time = 0 # Time when kernel is ready
+        self.starting_time = 0 # Time when kernel starts executing on the block
+        self.completion_time = 0 # Time when execution has completed
+        self.deadline = 0.
+        # self.min_time = self.get_best_case_estimated_latency()  # WCET
+        # self.max_time = self.get_worst_case_estimated_latency() # BCET
+
+        self.min_time = None
+        self.max_time = None
+
+        self.best_energy, self.best_energy_pe = self.get_best_case_energy_if_krnel_run_in_isolation()
+        self.worst_energy,self.worst_energy_pe = self.get_worst_case_energy_if_krnel_run_in_isolation()
 
         # Shows what block is the bottleneck at every phase of the kernel execution
         self.kernel_phase_bottleneck_blocks_dict = defaultdict(dict)
         self.block_num_shared_blocks_dict = {}  # how many other kernels shared this block
-        #self.work_unit_dict = self.__task_to_blocks_map.task.__task_to_family_task_work_unit  # determines for each burst how much work needs
+        #self.work_unit_dict = self.task.__task_to_family_task_work_unit  # determines for each burst how much work needs
                                                                                          # to be done.
         self.path_structural_latency = {}
 
+        self.possible_block_instance_name = None
+
         self.firing_time_to_meet_throughput = {}
         self.firing_work_to_meet_throughput = {}
         self.data_work_left_to_meet_throughput = {}
 
 
+    def print_blocks_dir_work_ratio(self): 
+        print(f">> Task {self.task_name}")
+        for b_d, wr_dict in self.__task_to_blocks_map.block_dir_workRatio_dict.items():
+            print(f">>\tblock {b_d[0].instance_name} {b_d[1]}, {[(b_,wr) for b_,wr in wr_dict.items()]}")
+
+    def update_blocks_dir_work_ratio(self, blocks_dir_work_ratio:Dict[Tuple[Block, str], float]={}):
+        self.__task_to_blocks_map.block_dir_workRatio_dict = blocks_dir_work_ratio
+
     # This function is used for the power knobs simulator; After each run the stats
     #  gathered for each kernel is removed to avoid conflicting with past simulations
     def reset_sim_stats(self):
         # status vars
         self.cur_phase_bottleneck = ""
         self.block_att_work_rate_dict = defaultdict(dict)
-        self.stats = KernelStats()
+        self.stats = KernelStats(self.__task_to_blocks_map.task.is_task_dummy(), self.task_name)
         self.workload_pe_total_work, self.workload_fraction, self.pe_s_work_left, self.progress, self.status = [None]*5
         self.phase_num = -1 # since the very first time, doesn't count
         self.block_normalized_work_rate = {}  # blocks and their work_rate (not normalized) including sharing
@@ -493,8 +569,8 @@
         self.task_name = self.__task_to_blocks_map.task.name
         self.work_unit_dict = self.__task_to_blocks_map.task.__task_to_family_task_work_unit  # determines for each burst how much work needs
         self.power_knob_id = 0
-        self.starting_time = 0
-        self.completion_time = 0
+        self.starting_time = 0.
+        self.completion_time = 0.
         self.kernel_phase_bottleneck_blocks_dict = defaultdict(dict)
         self.block_num_shared_blocks_dict = {}
         self.operating_state = "none"
@@ -524,10 +600,10 @@
     # --------------
     # getters
     # --------------
-    def get_task(self):
+    def get_task(self) -> Task:
         return self.__task_to_blocks_map.task
 
-    def get_task_name(self):
+    def get_task_name(self) -> str:
         return self.__task_to_blocks_map.task.name
 
     # work here is PE's work, so specified in terms of number of instructions
@@ -535,27 +611,45 @@
         return self.kernel_total_work["execute"]
 
     # get the list of blocks the kernel uses
-    def get_block_list_names(self):
+    def get_block_list_names(self) -> List[str]:
         return [block.instance_name for block in self.__task_to_blocks_map.get_blocks()]
 
-    def get_blocks(self):
+    def get_blocks(self) -> List[Block]:
         return self.__task_to_blocks_map.get_blocks()
 
+    def get_task_to_blocks_map(self):
+        return self.__task_to_blocks_map
+
+    def get_ic_blocks(self) -> List[Block]:
+        ic_blocks = []
+        for block in self.__task_to_blocks_map.get_blocks():
+            if block.type == "ic":
+                ic_blocks.append(block)
+        return ic_blocks
+
+    def get_mem_blocks(self) -> List[Block]:
+        mem_blocks = []
+        for block in self.__task_to_blocks_map.get_blocks():
+            if block.type == "mem":
+                mem_blocks.append(block)
+        return mem_blocks
+
     # get the reference block for the kernel (reference block is what work rate is calculated
     # based off of.
-    def get_ref_block(self):
+    def get_ref_block(self) -> Block:
         for block in self.__task_to_blocks_map.get_blocks():
             if block.type == "pe":
                 return block
 
     # get kernel's memory blocks
     # dir: direction of interest (read/write)
-    def get_kernel_s_mems(self, dir):
+    def get_kernel_s_mems(self, dir) -> List[Block]:
         mems = []
         for block in self.__task_to_blocks_map.get_blocks():
             if block.type == "mem":
                 task_dir = block.get_task_dir_by_task_name(self.get_task())
                 for task, dir_ in task_dir:
+                    # print(f"task {task.name} dir {dir_}, this task name {self.get_task_name()}")
                     if dir_ == dir:
                         mems.append(block)
                         break
@@ -685,7 +779,7 @@
         # helper function
         def pipes_serial_work_ratio(pipe, scheduled_kernels, mode = "equal_per_kernel"):
             mode = "proportional_to_kernel"
-            #mode = "equal_per_kernel"
+            # mode = "equal_per_kernel"
             if mode == "equal_per_kernel":
                 num_tasks_present = 0
                 num_tasks_present = sum([int(pipe.is_task_present(krnel.__task_to_blocks_map.task)) for krnel in scheduled_kernels])
@@ -713,16 +807,21 @@
             for pipe in pipes_with_traffic:
                 pipe_serial_work_rate = pipes_serial_work_ratio(pipe, scheduled_kernels)
                 allotted_work_rate += (1/len(pipes_with_traffic)) * pipe_serial_work_rate
+                # if self.task_name == "VitPre_x1_3_5":
+                #     print(f"    ?? [{self.task_name}] pipes_with_traffic {pipe.get_info()} allotted_work_rate += (1/{len(pipes_with_traffic)}) * {pipe_serial_work_rate} = {(1/len(pipes_with_traffic)) * pipe_serial_work_rate}")
         return allotted_work_rate
 
     # calculate the work rate (BW or IPC depending on the hardware block) of each kernel, while considering
     # sharing of the block across live kernels
     def calc_allotted_work_rate_relative_to_other_kernles(self, mode, pipe_cluster, scheduled_kernels):
+        # print(f"@@ calc_allotted_work_rate_relative_to_other_kernles(self, {mode}, {pipe_cluster.get_info()}, {scheduled_kernels})")
         assert(mode in ["equal_rate_per_kernel", "equal_rate_per_pipe"])
         if mode == "equal_rate_per_kernel":
-            return float(1./self.block_num_shared_blocks_dict[pipe_cluster.get_ref_block()][pipe_cluster.dir][pipe_cluster.get_unique_name()])
+            rel_work_rate = float(1./self.block_num_shared_blocks_dict[pipe_cluster.get_ref_block()][pipe_cluster.dir][pipe_cluster.get_unique_name()])
         elif mode == "equal_rate_per_pipe":
-            return self.alotted_work_rate_equal_pipe(pipe_cluster, scheduled_kernels)
+            rel_work_rate = self.alotted_work_rate_equal_pipe(pipe_cluster, scheduled_kernels)
+        # print(f'@@ rel_work_rate calculated as {work_rate}')
+        return rel_work_rate
 
     def get_block_family_tasks_in_use(self, block):
         blocks_family_members = self.__task_to_blocks_map.get_block_family_members_allocated(block.instance_name)
@@ -932,6 +1031,8 @@
         # (1) calculate the share of each kernel for the channel. (2) get their work ratio to normalize to
         # (3) use peak rate, share of each kernel and work ratio to generate final results
         for block in self.get_blocks():
+            # print(f"@@ XXXXXXXXX BLOCK {block.instance_name} XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX")
+            # block.get_tasks_dir_work_ratio_for_printing()
             for pipe_cluster in block.get_pipe_clusters_of_task(self.get_task()):
                 if self.get_task().is_task_dummy():
                     block_work_rate_norm_dict[block][pipe_cluster] = 1
@@ -954,12 +1055,10 @@
                 else:
                     work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_family_member_names_and_channel_eliminating_fake(
                         block.instance_name, self.get_block_family_tasks_in_use(block), dir)
+                # print(f"@@ WORK RATIO CALC --> block = {block.instance_name}, self.get_block_family_tasks_in_use(block) = {[(t[0], t[1]) for t in self.get_block_family_tasks_in_use(block)]} dir = {dir}, work_ratio = {work_ratio}")
 
-                if work_ratio == 0:
-                    print("this should be looked at")
-                    work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_family_member_names_and_channel_eliminating_fake(
-                        block.instance_name, self.get_block_family_tasks_in_use(block), dir)
-
+                if work_ratio == 0: continue
+                assert work_ratio > 0, f"work_ratio for kernel {self.task_name} is {work_ratio}; this needs to be looked at"
 
                 # queue impact
                 #queue_impact = self.get_queue_impact(block, pipe_cluster, scheduled_kernels)
@@ -968,19 +1067,55 @@
 
                 work_rate =  queue_impact*float(block.get_peak_work_rate(self.get_power_knob_id()))*allocated_work_rate_relative_to_other_kernels/work_ratio
                 block_work_rate_norm_dict[block][pipe_cluster] = work_rate
+                # if self.task_name == "VitPre_x1_3_5":
+                #     print(f"-- block_work_rate_norm_dict[{block.instance_name}][{pipe_cluster.ref_block.instance_name},{pipe_cluster.dir}] = peak_wr ({float(block.get_peak_work_rate(self.get_power_knob_id()))}) * allocated_work_relative ({allocated_work_rate_relative_to_other_kernels}) / work_ratio {work_ratio} = {work_rate}")
 
         return block_work_rate_norm_dict
 
     # simply go through all the block work rate and pick the smallest
     def calc_block_s_bottleneck(self, block_work_rate_norm_dict):
+        # Per-kernel dictionary of blocks to pipe cluster directions to work rates.
+        task_stats_dict = {
+            'blocks':               {}, 
+            'bottleneck':           None, 
+            'bottleneck_work_rate': None, 
+            'comp_work_rate':       None,
+            'mem_work_rate':        np.inf, # includes memory and ic
+            'mem_bandwidth':        None,   # includes memory and ic
+            }
         # only if work unit is left
         block_bottleneck = {"write": None, "read":None}
         bottleneck_work_rate = {"write": np.Inf, "read":np.Inf}
         # iterate through all the blocks/channels and ge the minimum work rate. Since
         # the data is normalized, minimum is the bottleneck
         for block, pipe_cluster_work_rate in block_work_rate_norm_dict.items():
+            # print(f"\n@@ >> block {block.instance_name}")
             for pipe_cluster, work_rate in pipe_cluster_work_rate.items():
                 dir_ = pipe_cluster.get_dir()
+                if block.type == "pe":
+                    assert task_stats_dict['comp_work_rate'] == None
+                    task_stats_dict['comp_work_rate'] = work_rate  # For normalization with the PE work rate.
+                    
+                else:
+                    if work_rate < task_stats_dict['mem_work_rate']:
+                        task_stats_dict['mem_work_rate'] = work_rate
+                        # Re-calculate allocated bandwidth
+                        work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_family_member_names_and_channel_eliminating_fake(
+                            block.instance_name, self.get_block_family_tasks_in_use(block), dir_)
+                        assert not work_ratio == 0
+                        task_stats_dict['mem_bandwidth'] = work_rate * work_ratio
+                        # print(f"@@ WORK RATIO RE-CALC --> block = {block.instance_name}, dir = {dir_}, work_ratio = {work_ratio}, bw = {work_rate * work_ratio}")
+
+                # print(f">>\tdir {dir_}, pipe_cluster {pipe_cluster.get_info()}, work_rate {work_rate:,}")
+
+                # phase -> block -> work rate
+                if block.instance_name not in task_stats_dict['blocks']:
+                    task_stats_dict['blocks'][block.instance_name] = {}
+                # print(f"[STATS] Updating stats dict for kernel {self.task_name}, phase {self.phase_num}, block {block.instance_name}, dir {dir_}")
+                dir_pc_unique_name = f"{dir_}_{pipe_cluster.unique_name}"
+                assert not dir_pc_unique_name in task_stats_dict['blocks'][block.instance_name], f"Entry for {dir_pc_unique_name} already present in dict task_stats_dict['blocks'][{block.instance_name}]: {task_stats_dict['blocks'][block.instance_name]}"
+                task_stats_dict['blocks'][block.instance_name][dir_pc_unique_name] = work_rate
+
                 if dir_ == "same":  # same is for PEs. In that case, we will apply it for both the read and write path
                     dirs = ["write", "read"]
                 else:
@@ -994,8 +1129,18 @@
                             bottleneck_work_rate[dir__] = work_rate
                             block_bottleneck[dir__] = block
 
-        return block_bottleneck, bottleneck_work_rate
-
+        block_bottleneck_ = list(block_bottleneck.values())[0]
+        bottleneck_work_rate_ = list(bottleneck_work_rate.values())[0]
+        # print(f"@@ task {self.task_name}, block_bottleneck = {block_bottleneck_.instance_name}, bottleneck_work_rate = {bottleneck_work_rate_}")
+        task_stats_dict['bottleneck'] = block_bottleneck_
+        task_stats_dict['bottleneck_work_rate'] = bottleneck_work_rate_  # For normalization with the PE work rate.
+        assert task_stats_dict['comp_work_rate'] != None
+        assert not np.isinf(task_stats_dict['mem_work_rate'])
+        # print(f"@@ comp_work_rate = {task_stats_dict['comp_work_rate']}, mem_work_rate = {task_stats_dict['mem_work_rate']}")
+        # if self.task_name == "VitPre_x1_3_5":
+        #     for d, bottleneck_block in block_bottleneck.items():
+        #         print(f"@@ kernel {self.task_name} block bottleneck for phase {self.phase_num} in dir {d} is {bottleneck_block.instance_name} with work rate {bottleneck_work_rate[d]}")
+        return block_bottleneck, bottleneck_work_rate, task_stats_dict
 
     # calculate the unnormalized work rate.
     # Normalization is the process of normalizing the work_rate of each block with respect of the
@@ -1039,6 +1184,7 @@
 
                 if "souurce" in self.get_task_name() or "siink" in self.get_task_name() or config.sw_model == "sequential":
                     work_ratio = 1
+                # print(f"@@ normalizing block {block.instance_name}, pipe_cluster {pipe_cluster.get_info()}with work_ratio {work_ratio}, bottleneck_work_rate {bottleneck_work_rate}")
                 block_att_work_rate_dict[block][pipe_cluster] = bottleneck_work_rate*work_ratio
                 #self.update_pipe_cluster_work_rate(pipe_cluster, bottleneck_work_rate)
 
@@ -1046,6 +1192,7 @@
 
     # update paths (inpipe-outpipe) work rate
     def update_pipe_clusters_pathlet_work_rate(self):
+        # print(f"\n@@ update_pipe_clusters_pathlet_work_rate KERNEL {self.task_name}\n")
         for block in self.get_blocks():
             for pipe_cluster in block.get_pipe_clusters():
                 self.update_pipe_cluster_pathlet_work_rate(pipe_cluster, self.cur_phase_bottleneck_work_rate)
@@ -1191,7 +1338,7 @@
                         #self.path_dir_phase_latency[dir_][phase_num] += val
                         block = pathlet.get_in_pipe().get_slave()
                         pipe_cluster = pathlet.getma
-                        block_att_work_rate_dict[block][pipe_cluster]
+                        # block_att_work_rate_dict[block][pipe_cluster]
                         self.path_dir_phase_latency[dir_][phase_num] = 64/self.block_att_work_rate_dict[pathlet.get_in_pipe().get_slave()]
 
     # update paths (inpipe-outpipe) work rate
@@ -1224,6 +1371,7 @@
 
                     work_unit = self.get_task().get_smallest_task_work_unit_by_dir(dir)
                     self.block_path_dir_phase_latency[block][pipe_cluster][dir][last_phase] =  work_unit/ work_rate
+                    # print(f"@@ ||||| self.block_path_dir_phase_latency[{block.instance_name}][{pipe_cluster.get_info()}][{dir}][{last_phase}] =  {work_unit} (work_unit)/ {work_rate} (work_rate)")
 
     # not considering congestion in the system
     # calculate the path latency for a kernel
@@ -1241,7 +1389,7 @@
         for pe in pes:
             for mem in read_mems:
                 dir = "read"
-                path = design.get_hardware_graph().get_path_between_two_vertecies(pe, mem)
+                path = design.get_hardware_graph().get_path_between_two_vertices(pe, mem)
                 hop_latency = 0
                 # hop latency
                 for blk in path:
@@ -1259,7 +1407,7 @@
         for pe in pes:
             for mem in read_mems:
                 dir = "write"
-                path = design.get_hardware_graph().get_path_between_two_vertecies(pe, mem)
+                path = design.get_hardware_graph().get_path_between_two_vertices(pe, mem)
                 hop_latency = 0
                 # hop latency
                 for blk in path:
@@ -1278,7 +1426,7 @@
     # update paths (inpipe-outpipe) work rate
     # We are not using the following as it was not usefull (and with verification didn't give us good results)
     def update_pipe_clusters_pathlet_latency(self, scheduled_kernels):
-        if "souurce" in self.get_task_name() or "siink" in self.get_task_name() or "dummy_last" in self.get_task_name():
+        if self.get_task().is_task_dummy():
             return
         for block in self.get_blocks():
             for pipe_cluster in block.get_pipe_clusters():
@@ -1381,15 +1529,80 @@
         return block_normalized_work_rate_consolidated
 
 
-    def get_latency_if_krnel_run_in_isolation(self):
+    def get_latency_if_krnel_run_in_isolation(self) -> Tuple[float, Dict]:
         if self.get_task().is_task_dummy():
-            return 0
+            return 0, None
         if config.sw_model == "sequential":
             print("can not do kerel calculation in isolation for sequential mode yet")
-            return .1
-        block_att_work_rate_dict = self.update_block_att_work_rate_in_isolation()
+            return .1, None
+        block_att_work_rate_dict, task_stats_dict = self.update_block_att_work_rate_in_isolation()
         time_to_completion_in_isolation = self.get_total_work()/block_att_work_rate_dict[self.get_ref_block()][self.get_ref_block().get_pipe_clusters()[0]]
-        return time_to_completion_in_isolation
+        return time_to_completion_in_isolation, task_stats_dict
+
+    def get_comp_latency_if_krnel_run_in_isolation(self, block:Block) -> float:
+        if self.get_task().is_task_dummy():
+            return 0
+
+        # assert self.blocks
+        # assert block in self.blocks
+        # # consider this block only if it is a PE and the task can run on it
+        # assert block.get_block_type_name() == "pe"
+        # assert block.instance_name.split('_pe')[0] in self.task_to_mappable_pe_map[self.get_task_name()]
+        # assert self.kernel_total_work != 0
+
+        b_wr = float(block.get_peak_work_rate())
+        lat = self.kernel_total_work["execute"] / b_wr # in s
+        return lat
+
+    def get_energy_if_krnel_run_in_isolation(self, block:Block) -> float:
+        if self.get_task().is_task_dummy():
+            return 0
+        b_wr = float(block.get_peak_work_rate())
+        exec_time = self.kernel_total_work["execute"] / b_wr
+        energy = self.calc_energy_consumed_by_block_given_time(block, exec_time)
+        return energy
+
+    def get_best_case_energy_if_krnel_run_in_isolation(self) -> Tuple[float, Block]:
+        if self.get_task().is_task_dummy():
+            return 0, None
+
+        assert self.blocks
+        assert self.kernel_total_work["execute"] != 0
+
+        # get block with fastest work rate for this task and calculate best case latency
+        best_energy, best_energy_pe = np.inf, None
+        for b in self.blocks:
+            # consider this block only if it is a PE and the task can run on it
+            if b.get_block_type_name() != "pe" or b.instance_name.split('_pe')[0] not in self.task_to_mappable_pe_map[self.get_task_name()]:
+                continue
+            b_wr = float(b.get_peak_work_rate())
+            exec_time = self.kernel_total_work["execute"] / b_wr
+            energy = self.calc_energy_consumed_by_block_given_time(b, exec_time)
+            if energy < best_energy:
+                best_energy = energy
+                best_energy_pe = b
+        return best_energy, best_energy_pe
+
+    def get_worst_case_energy_if_krnel_run_in_isolation(self) -> Tuple[float, Block]:
+        if self.get_task().is_task_dummy():
+            return 0, None
+
+        assert self.blocks
+        assert self.kernel_total_work["execute"] != 0
+
+        # get block with fastest work rate for this task and calculate worst case latency
+        worst_energy, worst_energy_pe = 0., None
+        for b in self.blocks:
+            # consider this block only if it is a PE and the task can run on it
+            if b.get_block_type_name() != "pe" or b.instance_name.split('_pe')[0] not in self.task_to_mappable_pe_map[self.get_task_name()]:
+                continue
+            b_wr = float(b.get_peak_work_rate())
+            exec_time = self.kernel_total_work["execute"] / b_wr
+            energy = self.calc_energy_consumed_by_block_given_time(b, exec_time)
+            if energy > worst_energy:
+                worst_energy = energy
+                worst_energy_pe = b
+        return worst_energy, worst_energy_pe
 
     # only for one krnel
     def update_block_att_work_rate_in_isolation(self):
@@ -1400,7 +1613,7 @@
         block_normalized_work_rate = self.consolidate_channels(block_normalized_work_rate_unconsolidated)
 
         # identify the block bottleneck
-        cur_phase_dir_bottleneck, cur_phase_dir_bottleneck_work_rate = self.calc_block_s_bottleneck(block_normalized_work_rate)
+        cur_phase_dir_bottleneck, cur_phase_dir_bottleneck_work_rate, tasks_stat_dict = self.calc_block_s_bottleneck(block_normalized_work_rate)
         cur_phase_bottleneck = list(cur_phase_dir_bottleneck.values())[0]
         cur_phase_bottleneck_work_rate = list(cur_phase_dir_bottleneck_work_rate.values())[0]
         for dir, work_rate in cur_phase_dir_bottleneck_work_rate.items():
@@ -1415,13 +1628,11 @@
         block_att_work_rate_dict = self.calc_unnormalize_work_rate(block_normalized_work_rate, cur_phase_bottleneck_work_rate)
         block_dir_att_work_rate_dict = self.calc_unnormalize_work_rate_by_dir(block_normalized_work_rate, cur_phase_dir_bottleneck_work_rate)
 
-        return block_att_work_rate_dict
+        return block_att_work_rate_dict, tasks_stat_dict
 
 
     # calculate the attainable work rate of the block
     def update_block_att_work_rate(self, scheduled_kernels):
-
-        #
         scheduled_kernels_tmp = []
         if config.sw_model == "sequential":
             krnls_operating_state = self.operating_state
@@ -1438,7 +1649,7 @@
         self.block_normalized_work_rate = self.consolidate_channels(self.block_normalized_work_rate_unconsolidated)
 
         # identify the block bottleneck
-        cur_phase_dir_bottleneck, cur_phase_dir_bottleneck_work_rate = self.calc_block_s_bottleneck(self.block_normalized_work_rate)
+        cur_phase_dir_bottleneck, cur_phase_dir_bottleneck_work_rate, tasks_stat_dict = self.calc_block_s_bottleneck(self.block_normalized_work_rate)
         self.cur_phase_bottleneck = list(cur_phase_dir_bottleneck.values())[0]
         self.cur_phase_bottleneck_work_rate = list(cur_phase_dir_bottleneck_work_rate.values())[0]
         for dir, work_rate in cur_phase_dir_bottleneck_work_rate.items():
@@ -1452,6 +1663,7 @@
         # to normalizing it to the ref block (which is usally PE) work rate
         self.block_att_work_rate_dict = self.calc_unnormalize_work_rate(self.block_normalized_work_rate, self.cur_phase_bottleneck_work_rate)
         self.block_dir_att_work_rate_dict = self.calc_unnormalize_work_rate_by_dir(self.block_normalized_work_rate, cur_phase_dir_bottleneck_work_rate)
+        return tasks_stat_dict
 
 
     def get_completion_time(self):
@@ -1512,6 +1724,8 @@
                 self.block_dir_work_left[block_dir_work_ratio][task] = (self.pe_s_work_left*ratio)
 
         self.status = "in_progress"
+        self.get_ref_block().busy = True
+        self.get_ref_block().kernel_running = self
         if self.iteration_ctr == self.max_iteration_ctr:
             self.starting_time = cur_time
         self.update_krnl_iteration_ctr()
@@ -1627,7 +1841,8 @@
 
         self.stats.latency += time_step_size
 
-        if self.progress >= .99:
+        if self.progress >= .9999999:
+        # if math.isclose(self.progress, 1.):
             self.status = "completed"
             #self.completion_time = self.stats.latency + self.starting_time
             self.completion_time = clock
@@ -1654,9 +1869,10 @@
     def calc_work_consumed(self, time_step_size):
 
         # iterate through each blocks attainable work rate and calculate
-        # initialize
-        for block in self.get_blocks():
-            self.block_phase_work_dict[block][self.phase_num] = 0
+        # # initialize
+        # print(f"@@ phase {self.phase_num}, self.get_blocks() = {[b.instance_name for b in self.get_blocks()]}")
+        # for block in self.get_blocks():
+        #     self.block_phase_work_dict[block][self.phase_num] = 0
 
         # how much work it can do for the kernel of interest
         for block, pipe_clusters_work_rate in self.block_att_work_rate_dict.items():
@@ -1669,9 +1885,9 @@
                     self.block_phase_work_dict[block][self.phase_num] = work_rate* time_step_size
 
                 if pipe_cluster.dir == "read":
-                     read_work += work_rate * time_step_size
+                    read_work += work_rate * time_step_size
                 if pipe_cluster.dir == "write":
-                     write_work += work_rate * time_step_size
+                    write_work += work_rate * time_step_size
 
             if self.get_type() == "throughput_based":
                 unit_time_to_meet_throughput = self.throughput_info["clock_period"] * (10 ** -9)
@@ -1690,12 +1906,17 @@
                 self.block_phase_write_dict[block][self.phase_num] += write_work
             else:
                 self.block_phase_write_dict[block][self.phase_num] = write_work
+            # print(f"@@ =========================")
+            # print(f"@@ block_phase_work_dict[{block.instance_name}][{self.phase_num}] = {self.block_phase_work_dict[block][self.phase_num]}")
+            # print(f"@@ block_phase_read_dict[{block.instance_name}][{self.phase_num}] = {self.block_phase_read_dict[block][self.phase_num]}")
+            # print(f"@@ block_phase_write_dict[{block.instance_name}][{self.phase_num}] = {self.block_phase_write_dict[block][self.phase_num]}")
+            # print(f"@@ =========================")
 
 
     # Calculates the leakage power of the phase for PE and IC
     # memory leakage power should be accumulated for the whole execution time
     # since we cannot turn off the memory but the rest can be in cut-off (C7) mode
-    def calc_leakage_energy_consumed(self, time_step_size):
+    def calc_leakage_energy_consumed(self, time_step_size:float):
         for block, work in self.block_phase_work_dict.items():
             # taking care of dummy corner case
             if "souurce" in self.get_task_name() or "siink" in self.get_task_name():
@@ -1704,12 +1925,17 @@
                 if block.get_block_type_name() == "mem":
                     self.block_phase_leakage_energy_dict[block][self.phase_num] = 0
                 else:
+                    leak_pwr = block.get_leakage_power(self.get_power_knob_id())
+                    if leak_pwr == "":
+                        assert block.type == "ic" # only expecting no static power data for IC
+                        leak_pwr = 0.
                     # changed to get by Hadi
-                    self.block_phase_leakage_energy_dict[block][self.phase_num] = \
-                        block.get_leakage_power(self.get_power_knob_id()) * time_step_size
+                    # print(block.instance_name, block.get_leakage_power(self.get_power_knob_id()), time_step_size)
+                    self.block_phase_leakage_energy_dict[block][self.phase_num] = leak_pwr * time_step_size
+            # print(f"Phase: {self.phase_num}: block: {block.instance_name}, leakage_energy: {self.block_phase_leakage_energy_dict[block][self.phase_num]}")
 
     # calculate energy consumed
-    def calc_energy_consumed(self):
+    def calc_energy_consumed(self, time_step_size:float):
         for block, work in self.block_phase_work_dict.items():
             # Dynamic energy consumption
             if "souurce" in self.get_task_name() or "siink" in self.get_task_name():  # taking care of dummy corner case
@@ -1720,11 +1946,21 @@
                 if this_phase_energy < 0:
                     print("energy can't be a negative value")
                     block.get_work_over_energy(self.get_power_knob_id())
-                    exit(0)
+                    exit(1)
 
                 self.block_phase_energy_dict[block][self.phase_num] = this_phase_energy
-
-                pass
+                # add static energy for the block here itself
+                blk_leak_energy = self.block_phase_leakage_energy_dict[block][self.phase_num]
+                if block.type == "pe": # we don't have run-time info on memory sizes and thus on memory leakage power, so this is unaccounted for until end-of-sim; IC static power is assumed 0 
+                    assert blk_leak_energy > 0 or time_step_size == 0., f"Leakage energy for block {block.instance_name}, phase {self.phase_num} is {blk_leak_energy}"
+                self.block_phase_energy_dict[block][self.phase_num] += blk_leak_energy
+                # print(f"@@ Setting -> self.block_phase_energy_dict[{block.instance_name}][{self.phase_num}] = {self.block_phase_energy_dict[block][self.phase_num]}")
+    
+    def calc_energy_consumed_by_block_given_time(self, block, exec_time):
+        dynamic_energy = float(block.get_peak_work_rate(self.get_power_knob_id())) * exec_time / block.get_work_over_energy(self.get_power_knob_id())
+        leakage_energy = float(block.get_leakage_power(self.get_power_knob_id())) * exec_time
+        
+        return dynamic_energy + leakage_energy
 
     # for read, we release memory (the entire input worth of data) once the kernel is done
     # for write, we assign memory (the entire output worth of data) once the kernel starts
@@ -1732,39 +1968,46 @@
     # go of that till the sibling is done
     # coeff is gonna determine whether to retract or expand the memory
     # Todo: include the case where there are multiple siblings
-    def update_mem_size(self, coef):
-        if "souurce" in self.get_task_name() and coef == -1: return
-        elif "siink" in self.get_task_name() and coef == 1: return
+    def update_mem_size(self, coef:int, task_name:str, task_s_mems:List[Block], cur_dag_id:int=None, children_of_souurce={}, task_requesting=None):
+        if "souurce" in task_name and coef == -1: return
+        elif "siink" in task_name and coef == 1: return
 
         dir_ = "write"
-        mems = self.get_kernel_s_mems(dir=dir_)
-        if "souurce" in self.get_task_name():
+
+        if "souurce" in task_name:
+            assert cur_dag_id != None, "Need cur_dag_id to be set for souurce"
+            assert children_of_souurce
             if dir_ == "write":
+                assert task_requesting != None
                 #memory_total_work = config.souurce_memory_work
-                for mem in mems:
-                    # mem_work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_dir(mem.instance_name, dir_)
-                    tasks_name = self.__task_to_blocks_map.get_tasks_of_block_dir(
-                        mem.instance_name, dir_)
-                    # Sends the memory size that needs to be occupied (positive for write and negative for read)
-                    # then updates the memory mapping in the memory block to know what is the in use capacity
-                    # changed to get by Hadi
-                    for tsk in tasks_name:
-                        memory_total_work = config.souurce_memory_work[tsk]
-                        mem.update_area(coef*memory_total_work/mem.get_work_over_area(self.get_power_knob_id()), self.get_task_name())
-                        mem.update_area_in_bytes(coef*memory_total_work, self.get_task_name())
-            # mem.update_area(coef*memory_total_work/mem.get_work_over_area(self.get_power_knob_id()), self.get_task_name())
+                for mem in task_s_mems:
+                    if task_requesting in children_of_souurce[mem]:
+                        # mem_work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_dir(mem.instance_name, dir_)
+                        # Sends the memory size that needs to be occupied (positive for write and negative for read)
+                        # then updates the memory mapping in the memory block to know what is the in use capacity
+                        # changed to get by Hadi
+                        # print(f"&& souurce, mem {mem.instance_name}, task_requesting = {task_requesting}")
+                        tsk_dag_id = int(task_requesting.split('_')[-1])
+                        # filter out tasks which don't belong to DAG cur_dag_id
+                        if tsk_dag_id != cur_dag_id:
+                            continue
+                        memory_total_work = config.souurce_memory_work[task_requesting]
+                        mem.update_area(coef*memory_total_work/mem.get_work_over_area(self.get_power_knob_id()), task_name)
+                        mem.update_area_in_bytes(coef*memory_total_work, task_name)
+            # mem.update_area(coef*memory_total_work/mem.get_work_over_area(self.get_power_knob_id()), task_name)
             else: memory_total_work = 0
-        elif "siink" in self.get_task_name():
+        elif "siink" in task_name:
             memory_total_work = 0
         else:
             pe_s_total_work = self.kernel_total_work["execute"]
-            for mem in mems:
+            for mem in task_s_mems:
                 #mem_work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_dir(mem.instance_name, dir_)
                 mem_work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_dir_eliminating_fake(mem.instance_name, dir_)
                 memory_total_work = pe_s_total_work * mem_work_ratio
                 # changed to get by Hadi
-                mem.update_area_in_bytes(coef*memory_total_work, self.get_task_name())
-                mem.update_area(coef*memory_total_work/mem.get_work_over_area(self.get_power_knob_id()), self.get_task_name())
+                # print(f"&& {task_name}, mem {mem.instance_name}")
+                mem.update_area_in_bytes(coef*memory_total_work, task_name)
+                mem.update_area(coef*memory_total_work/mem.get_work_over_area(self.get_power_knob_id()), task_name)
 
                 mem_work_ratio = self.__task_to_blocks_map.get_workRatio_by_block_name_and_dir(mem.instance_name, dir_)
 
@@ -1775,6 +2018,7 @@
         # DSPs and Processors are among statically sized blocks while accelerators are not among the list
         if pe.subtype in config.statically_sized_blocks: work = 1
         else: work = self.kernel_total_work["execute"]
+        # print(f"&& updated pe area to {work/pe.get_work_over_area(self.get_power_knob_id())}")
         pe.update_area(work/pe.get_work_over_area(self.get_power_knob_id()), self.get_task_name())
 
     # TODO: need to include ic as well
@@ -1843,6 +2087,8 @@
     def aggregate_energy_of_for_every_phase(self):
         aggregate_phase_energy = {}
         for block, phase_energy_dict in self.block_phase_energy_dict.items():
+            # print(f"AGG {block.instance_name}")
+            # pprint(phase_energy_dict)
             for phase, energy in phase_energy_dict.items():
                 this_phase_energy = phase_energy_dict[phase]
                 if phase not in aggregate_phase_energy.keys():
@@ -1895,8 +2141,8 @@
         # calculate the metric consumed for each phase
         self.calc_work_consumed(time_step_size)
         self.calc_work_left()
-        self.calc_energy_consumed()
-        #self.calc_leakage_energy_consumed(time_step_size)
+        self.calc_leakage_energy_consumed(time_step_size)
+        self.calc_energy_consumed(time_step_size)
 
         # Calculates the leakage power of the phase for PE and IC
         # memory leakage power should be accumulated for the whole execution time
@@ -1908,6 +2154,7 @@
     def update_stats(self, time_step_size):
         self.stats.phase_block_duration_bottleneck[self.phase_num] = (self.cur_phase_bottleneck, time_step_size)
         self.stats.phase_energy_dict[self.phase_num] = self.aggregate_energy_of_phase()
+        # print(f"self.stats.phase_energy_dict[{self.phase_num}] = {self.stats.phase_energy_dict[self.phase_num]}")
         self.stats.phase_latency_dict[self.phase_num] = time_step_size
         self.stats.block_phase_energy_dict = self.block_phase_energy_dict
         if config.simulation_method == "power_knobs":
@@ -1915,18 +2162,27 @@
             self.stats.phase_leakage_energy_dict[self.phase_num] = \
                 self.aggregate_leakage_energy_of_phase()
 
-            # Update the starting and completion time of the kernel -> used for power knob simulator
-            self.stats.starting_time = self.starting_time
-            self.stats.completion_time = self.completion_time
+        # Update the starting and completion time of the kernel
+        self.stats.arrival_time = self.arrival_time
+        self.stats.starting_time = self.starting_time
+        self.stats.completion_time = self.completion_time
 
     # step the kernel progress forward
     # Variables:
     #       phase_num: phase number
     def step(self, time_step_size, phase_num):
+        # print(f"SP kernel = {self.task_name}")
+        # wcet, wcet_blk = self.get_worst_case_comp_latency_if_krnel_run_in_isolation()
+        # if wcet_blk:
+        #     print(f"SP wcet = {wcet}, blk = {wcet_blk.instance_name}")
+        # bcet, bcet_blk = self.get_best_case_comp_latency_if_krnel_run_in_isolation()
+        # if bcet_blk:
+        #     print(f"SP bcet = {bcet}, blk = {bcet_blk.instance_name}")
+        # print(f"@@ $$$$$$$$$$$$$$$$ SIM step(ts_size={time_step_size}, phase={phase_num}) $$$$$$$$$$$$$")
         self.phase_num = phase_num
         # update the amount of work remaining per block
-        self.update_progress(time_step_size)
+        return self.update_progress(time_step_size)
 
 
     def get_schedule(self):
-        return self.__schedule
+        return self.__schedule
\ No newline at end of file
